{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Course Work 2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [
        "weauXTgWRax8",
        "k1Ij5gXlGUcD",
        "Tp2HqH1eRax-",
        "rwD4fCEYQM8u",
        "6bcPMDnHAL3E",
        "RecyGCHuN3i1",
        "TgXyYnfXOBKz",
        "U7XT-rorOQ3b",
        "g3WLrNyuRl_7",
        "aM1STmAmC3D_",
        "kJ11jXskjiPu",
        "azdWFQKjdpeQ",
        "4DAqA4jjc3jY",
        "7dMGbbHaddKl",
        "KmoHkBhjdhp2",
        "zy83G2K-wOiB",
        "0VyYhM5ghw-G",
        "EbpQw6sPu-Xh",
        "BGVy6ZovvDh8",
        "7nNgmKzPKD2S",
        "RtyQWS_ppBt5",
        "U05ZI-Hg-wZ8",
        "mH3lf3Tx_BbP",
        "1unTeaUd5v3E",
        "RrEsD6aG55hg",
        "OL2b0dLH1Doo",
        "K7AvNss76GWZ",
        "BYODTKjaD97b",
        "LPtUkQplRXeO",
        "yq8DeNbJT4q8",
        "YNOLWeynW1uF",
        "ImFbC2foHSkJ",
        "NTgNAxAC9SAa",
        "byMgFNyYdH8h",
        "Mwi03J4n2IFE",
        "dzxXyOyMItZm",
        "ymKIGNo02SR8",
        "P38V5WnsJIS_",
        "Q7hTgnslZjKt",
        "DZHFDd60eFmc",
        "tX5GG0JUIBkR",
        "qVchsXXwIS6C",
        "_NsOghEFJs2t",
        "RNWZuWRAA2rv",
        "QhVPY4rqKJHX",
        "fraFq0ddK25H",
        "-Hj17GbGLFfb",
        "qLfyDlOQN5gC",
        "rKVynIce_lqf",
        "utx2fvKHeO4_",
        "a40Hgf5be-H6",
        "x7qdetY2ed83",
        "tx1x8RMsQuYL",
        "u8h2lHZ6Q7NG",
        "UxG8zILRRM_0",
        "SLiIT6m6LB7_",
        "i8gRiZZrhUWE",
        "IC3NxQifPNbP"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lydia0409/Planets/blob/master/Course_Work_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weauXTgWRax8",
        "colab_type": "text"
      },
      "source": [
        "# **Section 0:Preliminary Work**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1Ij5gXlGUcD",
        "colab_type": "text"
      },
      "source": [
        "## **Section 0.0:** Explore the given data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp2HqH1eRax-",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.01:** Check data first"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbglcPtZ9UT0",
        "colab_type": "text"
      },
      "source": [
        "Initial Setup and read Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHrI6GVXhyWY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -U skorch"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmTuLl6QL-Uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "import sklearn.ensemble as ens\n",
        "import sklearn.svm as svm\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "#upload on Google Drive and mount the drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "0oCxEIDJRax_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Read data from Google Drive\n",
        "traing = pd.read_csv(\"/content/drive/My Drive/A50/train_set.csv\")\n",
        "valg = pd.read_csv(\"/content/drive/My Drive/A50/test_set.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ZdN_NReRMqJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check\n",
        "traing.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROLOpSIiRRr1",
        "colab_type": "text"
      },
      "source": [
        "Here we find that there is a column of unnamed variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrS3eHb5RayC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# As 1st column is unknown parameters, we remove it\n",
        "traing.drop(columns = 'Unnamed: 0', inplace=True)\n",
        "valg.drop(columns = 'Unnamed: 0', inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ocnwIbURdf6",
        "colab_type": "text"
      },
      "source": [
        "Check if there are zeros"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zUxlIpQOvqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check zeros in dataframe\n",
        "(traing == 0).astype(int).sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLBGlKE0Rnmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check nans in dataframe\n",
        "traing.isnull().sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3W_YUEiE5Nz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traing.info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "etNLAxxNAtHL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check variables' formats and features\n",
        "for i in range(0,7):\n",
        "  print(traing[traing.columns[i]].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cprc72deG27u",
        "colab_type": "text"
      },
      "source": [
        "Here we see that for some columns in dataframe, we have some variables in string format. Some of the columns have integer inputs. We need to convert those variables into categorical data in form of int64.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwD4fCEYQM8u",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.02:** Histogram of data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Svc_J1mP3hll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot histograms to check distribution of different variable\n",
        "fig = plt.figure(figsize=(10,50 ))\n",
        "for x in range(0,7):\n",
        "      #fix the position of plot in subplots\n",
        "      plt.subplot(7,1,x+1)\n",
        "      #Plot hist\n",
        "      plt.hist(traing[traing.columns[x]],bins=7)\n",
        "      # add title\n",
        "      plt.title(traing.columns[x])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPQ_mxatEF6g",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the data set is partially unbalanced, as number of 'unacc' data is much higher than the others.We need to use bootstrap to balance the dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bcPMDnHAL3E",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.03:** Bootstrap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQS_3vYGLWlV",
        "colab_type": "text"
      },
      "source": [
        "**Bootstrap for train set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJds2rF5JAul",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# BootStrap\n",
        "count_0, count_1, count_2, count_3 = traing.rating.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_0 = traing[traing['rating'] == 'unacc']\n",
        "df_1 = traing[traing['rating'] == 'acc']\n",
        "df_2 = traing[traing['rating'] == 'good']\n",
        "df_3 = traing[traing['rating'] == 'vgood']\n",
        "# see balance\n",
        "print(count_0, count_1, count_2, count_3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMyULINqlxzF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bootstrapping\n",
        "df_0_over = df_0.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_1_over = df_1.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_2_over = df_2.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_3_over = df_3.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "# concatenating all classes again\n",
        "traing = pd.concat([df_0_over, df_1_over, df_2_over, df_3_over])\n",
        "traing = traing.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g7oJL_DLgCj",
        "colab_type": "text"
      },
      "source": [
        "**Bootstrap for test set**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyAtmanzJAES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "count_01, count_11, count_21, count_31 = valg.rating.value_counts()\n",
        "# Divide by class\n",
        "df_01 = valg[valg['rating'] == 'unacc']\n",
        "df_11 = valg[valg['rating'] == 'acc']\n",
        "df_21 = valg[valg['rating'] == 'good']\n",
        "df_31 = valg[valg['rating'] == 'vgood']\n",
        "# see balance\n",
        "print(count_01, count_11, count_21, count_31)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-xonZ8nI-bR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bootstrapping\n",
        "df_01_over = df_01.sample(count_11, replace=True).reset_index().drop(columns=['index'])\n",
        "df_11_over = df_11.sample(count_11, replace=True).reset_index().drop(columns=['index'])\n",
        "df_21_over = df_21.sample(count_11, replace=True).reset_index().drop(columns=['index'])\n",
        "df_31_over = df_31.sample(count_11, replace=True).reset_index().drop(columns=['index'])\n",
        "# concatenating all classes again\n",
        "valg = pd.concat([df_01_over, df_11_over, df_21_over, df_31_over])\n",
        "valg = valg.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnaUw90xN3pE",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.04:** Encoder and Standardisation of the descriptors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q6YyJRQTMId1",
        "colab_type": "text"
      },
      "source": [
        "**Encoder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCJ4LOlLAWr4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert categorical string variables into numerical variables with values corresponding to strings\n",
        "traing['buying1'] = traing['buying'].replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\n",
        "traing['maint1'] = traing['maint'].replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\n",
        "traing['doors1'] = traing['doors'].replace(2,0).replace(3,1).replace(4,2).replace('5more',3)\n",
        "traing['persons1'] = traing['persons'].replace(2,0).replace(4,1).replace('more',2)\n",
        "traing['lug_boot1'] = traing['lug_boot'] .replace('small',0).replace('med',1).replace('big',2)\n",
        "traing['safety1'] = traing['safety'].replace('low',0).replace('med',1).replace('high',2)\n",
        "traing['rating1'] = traing['rating'].replace('unacc',0).replace('acc',1).replace('good',2).replace('vgood',3)\n",
        "#do the same thing for validation set \n",
        "valg['buying1'] = valg['buying'].replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\n",
        "valg['maint1'] = valg['maint'].replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\n",
        "valg['doors1'] = valg['doors'].replace(2,0).replace(3,1).replace(4,2).replace('5more',3)\n",
        "valg['persons1'] = valg['persons'].replace(2,0).replace(4,1).replace('more',2)\n",
        "valg['lug_boot1'] = valg['lug_boot'] .replace('small',0).replace('med',1).replace('big',2)\n",
        "valg['safety1'] = valg['safety'].replace('low',0).replace('med',1).replace('high',2)\n",
        "valg['rating1'] = valg['rating'].replace('unacc',0).replace('acc',1).replace('good',2).replace('vgood',3)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKc4Cooq1VTm",
        "colab_type": "text"
      },
      "source": [
        "Replace commendatory string with high values and derogatory term with low values "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FCA85j4Mq7A",
        "colab_type": "text"
      },
      "source": [
        "**Standardisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvvYOzCvBnMf",
        "colab_type": "text"
      },
      "source": [
        "As the range of raw data may varies widely, some algorithms won't workd properly without standardization. In addition, the standardisation process will also make results converge much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uET_VWEGPZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import preprocessing\n",
        "#standardisation before adding columns' name of training data\n",
        "trainscalg = pd.DataFrame(preprocessing.scale(traing[['buying1','maint1','doors1','persons1','lug_boot1','safety1']]),columns = ['buying','maint','doors','persons','lug_boot','safety'])\n",
        "#standardisation before adding columns' name of validation data\n",
        "valscalg = pd.DataFrame(preprocessing.scale(valg[['buying1','maint1','doors1','persons1','lug_boot1','safety1']]),columns = ['buying','maint','doors','persons','lug_boot','safety'])\n",
        "#add label column which donot need to be standardised\n",
        "trainscalg['rating']=traing['rating1']\n",
        "valscalg['rating']=valg['rating1']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvgu8jlTIoUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split training and validation data\n",
        "X_traing = trainscalg[['buying','maint','doors','persons','lug_boot','safety']]\n",
        "Y_traing = trainscalg[['rating']]\n",
        "X_valg = valscalg[['buying','maint','doors','persons','lug_boot','safety']]\n",
        "Y_valg = valscalg[['rating']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFPfwCHeDvgA",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.05:**Try our data in simple RandomForest model and SVM model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Kc3yJosDePg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf_rf = RandomForestClassifier(n_estimators=100, max_depth=2,random_state=0)\n",
        "clf_rf.fit(X_traing,Y_traing)\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score( clf_rf.predict(X_traing), Y_traing)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score( clf_rf.predict(X_valg), Y_valg)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kYjLMqQtFJrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import SVM\n",
        "from sklearn import svm\n",
        "linear_svm = svm.SVC(kernel = 'linear')\n",
        "linear_svm.fit(X_traing,Y_traing)\n",
        "dec_linear = linear_svm.decision_function(X_traing)\n",
        "linear_svm_pred_train = linear_svm.predict(X_traing)\n",
        "linear_svm_pred_test = linear_svm.predict(X_valg)\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(linear_svm_pred_train, Y_traing)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(linear_svm_pred_test, Y_valg)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6s0y7aRFYoD",
        "colab_type": "text"
      },
      "source": [
        "Here we see, the models trained on our dataset are extremely bad with very low accuracy. Also two models trained by given data shows significant underfitting which is not resonable. The given data may still have some problems other than unbalance after bootstrap which means it is not necessary to continue our work with given data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gznj_qyua8i1",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.06:**Set CV to be stratified 5-fold "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w6JMO1gQUrmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xowze4rDbr6G",
        "colab_type": "text"
      },
      "source": [
        "## **Section 0.1:** Use the car data from UCI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xkkktv9qjn9q",
        "colab_type": "text"
      },
      "source": [
        "https://archive.ics.uci.edu/ml/datasets/Car+Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RecyGCHuN3i1",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.1.1:** Check Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3kpRJdkhmAC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "# Read car data from Google Drive\n",
        "car = pd.read_csv(\"/content/drive/My Drive/A50/car.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bgc8gQU0Hjgn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot histogram for rating variables\n",
        "plt.hist(car['rating'],bins=7)\n",
        "# add title\n",
        "plt.title('rating')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2dcBomp3Iei6",
        "colab_type": "text"
      },
      "source": [
        "Here we see, the new dataset is still unbalanced.We need to bootstrap dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TgXyYnfXOBKz",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.1.2:** Bootstrap and split data into training set and validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3egYBQffKLOx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# class count\n",
        "count_0, count_1, count_2, count_3 = car.rating.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_0 = car[car['rating'] == 'unacc']\n",
        "df_1 = car[car['rating'] == 'acc']\n",
        "df_2 = car[car['rating'] == 'good']\n",
        "df_3 = car[car['rating'] == 'vgood']\n",
        "# see balance\n",
        "print(count_0, count_1, count_2, count_3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ONwVAMKn45",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bootstrapping\n",
        "df_0_over = df_0.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_1_over = df_1.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_2_over = df_2.sample(count_1, replace=True).reset_index().drop(columns=['index'])\n",
        "df_3_over = df_3.sample(count_1, replace=True).reset_index().drop(columns=['index'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AX4YvFvmKs_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# concatenating all classes again\n",
        "car_over = pd.concat([df_0_over, df_1_over, df_2_over, df_3_over])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ug5wiIITLDZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_over = car_over[['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety']]\n",
        "y_over = car_over[['rating']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oChHP2SaLKDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Split data into training set and validation set\n",
        "X_train, X_val, Y_train, Y_val = sklearn.model_selection.train_test_split(X_over, y_over, test_size=0.2, random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zj3L07UXMA28",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Reset index\n",
        "X_train = X_train.reset_index(drop=True)\n",
        "X_val = X_val.reset_index(drop=True)\n",
        "Y_train = Y_train.reset_index(drop=True)\n",
        "Y_val = Y_val.reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9FMZVYFGYSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot histogram for rating variables\n",
        "plt.hist(Y_train['rating'],bins=7)\n",
        "# add title\n",
        "plt.title('rating')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71SQUfD6GhiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Plot histogram for rating variables\n",
        "plt.hist(Y_val['rating'],bins=7)\n",
        "# add title\n",
        "plt.title('rating')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRAiJa2tGj2O",
        "colab_type": "text"
      },
      "source": [
        "Here we find that both validation and training sets are balanced."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7XT-rorOQ3b",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.1.3:** Encoder and Standardization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_c69IBfOa0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check variables' formats and features\n",
        "for i in range(0,6):\n",
        "  print(X_train[X_train.columns[i]].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FS1MeZO2NcRN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#convert categorical string variables into numerical variables with values corresponding to strings\n",
        "X_train = X_train.replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\\\n",
        "                .replace('5more',5).replace('more',6).replace('small',0).replace('big',2)\n",
        "X_val = X_val.replace('low',0).replace('med',1).replace('high',2).replace('vhigh',3)\\\n",
        "                .replace('5more',5).replace('more',6).replace('small',0).replace('big',2)\n",
        "Y_train = Y_train.replace('unacc',0).replace('acc',1).replace('good',2).replace('vgood',3)\n",
        "Y_val = Y_val.replace('unacc',0).replace('acc',1).replace('good',2).replace('vgood',3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4wa_fpXz3bp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Check variables' formats and features\n",
        "for i in range(0,6):\n",
        "  print(X_val[X_val.columns[i]].value_counts())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "J2x-aE6-QzLv"
      },
      "source": [
        "**Standardisation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OsYYjn-uQ4bE"
      },
      "source": [
        "As the range of raw data may varies widely, some algorithms won't workd properly without standardization. In addition, the standardisation process will also make results converge much faster."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SK3VXRTDRX_L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here we only need to standardise X_train and X_val\n",
        "from sklearn import preprocessing\n",
        "X_train = pd.DataFrame(preprocessing.scale(X_train),columns = X_train.columns)\n",
        "X_val = pd.DataFrame(preprocessing.scale(X_val),columns = X_train.columns)\n",
        "#leave Y in form of int64"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3WLrNyuRl_7",
        "colab_type": "text"
      },
      "source": [
        "### **Section 0.1.4:** Train 2 simple models for our new training set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAsAG1_4eNAc",
        "colab_type": "text"
      },
      "source": [
        "**RandomForestClassifier**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNmnTNk-Rh5h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf_rf = RandomForestClassifier()\n",
        "clf_rf.fit(X_train,Y_train)\n",
        "\n",
        "# validation accuracy\n",
        "\n",
        "rf_pred_train = clf_rf.predict(X_train)\n",
        "rf_pred_val = clf_rf.predict(X_val)\n",
        "\n",
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(rf_pred_train, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(rf_pred_val, Y_val)*100)+'%')\n",
        "from sklearn.metrics import confusion_matrix\n",
        "confusion_matrix(rf_pred_val, Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5HE8ZwUR9yA",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the RandomForest model fits new data much better than original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w_NHq9Xef03",
        "colab_type": "text"
      },
      "source": [
        "Linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWVYHv35e0r4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import SVM\n",
        "from sklearn import svm\n",
        "linear_svm = svm.SVC(kernel = 'linear')\n",
        "linear_svm.fit(X_train,Y_train)\n",
        "dec_linear = linear_svm.decision_function(X_train)\n",
        "linear_svm_pred_train = linear_svm.predict(X_train)\n",
        "linear_svm_pred_test = linear_svm.predict(X_val)\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(linear_svm.predict(X_train), Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(linear_svm.predict(X_val), Y_val)*100)+'%')\n",
        "confusion_matrix(rf_pred_val, Y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6mQKyW_ggmH",
        "colab_type": "text"
      },
      "source": [
        "Here we see that, although the accuracy of LinearSVM is obviously smaller than accuracy of RandomForest,the  Linear SVM model fits the new data much better than original data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HMjDqKCogjb2",
        "colab_type": "text"
      },
      "source": [
        "### **In the following CourseWork, car data will be used for further exploration**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM1STmAmC3D_",
        "colab_type": "text"
      },
      "source": [
        "# **Section 1:Random Forest Classifier**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ11jXskjiPu",
        "colab_type": "text"
      },
      "source": [
        "## **Section 1.1:** Visualising different hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azdWFQKjdpeQ",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.1.1:** Define validation curve function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7T6tTHzqcmr8",
        "colab_type": "text"
      },
      "source": [
        "Define function for detail plot of validation curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1p-E4UqXqQi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import validation_curve\n",
        "def visualhyper(train_score,test_score,hyper,param):\n",
        "\n",
        "  # Calculate mean and standard deviation for training set scores\n",
        "  train_mean = np.mean(train_score, axis=1)\n",
        "  train_std = np.std(train_score, axis=1)\n",
        "\n",
        "  # Calculate mean and standard deviation for test set scores\n",
        "  test_mean = np.mean(test_scores, axis=1)\n",
        "  test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "  # Plot mean accuracy scores for training and test sets\n",
        "  plt.plot(param, train_mean, label=\"Training score\", color=\"darkorange\")\n",
        "  plt.plot(param, test_mean, label=\"Cross-validation score\", color=\"navy\")\n",
        "\n",
        "  # Plot accurancy bands for training and test sets\n",
        "  plt.fill_between(param, train_mean - train_std, train_mean + train_std, color=\"darkorange\",alpha=0.2)\n",
        "  plt.fill_between(param, test_mean - test_std, test_mean + test_std, color=\"navy\",alpha=0.2)\n",
        "  # Create plot\n",
        "  plt.title(\"Validation Curve Corresponding to {}\".format(hyper))\n",
        "  plt.xlabel(hyper)\n",
        "  plt.ylabel(\"Accuracy Score\")\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()\n",
        "  return [train_mean,test_mean];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VX9dOse4ctKV",
        "colab_type": "text"
      },
      "source": [
        "define function for plot of general validation curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBbysrV9-Urc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def visualhyper1(train_score,test_score,hyper,param):\n",
        "  train_scores_mean = np.mean(train_score, axis=1)\n",
        "  train_scores_std = np.std(train_score, axis=1)\n",
        "  test_scores_mean = np.mean(test_score, axis=1)\n",
        "  test_scores_std = np.std(test_score, axis=1)\n",
        "\n",
        "  plt.title(\"Validation Curve Corresponding to {}\".format(hyper))\n",
        "  plt.xlabel(hyper)\n",
        "  plt.ylabel(\"Accuracy Score\")\n",
        "  plt.ylim(0.0, 1.1)\n",
        "  lw = 2\n",
        "  plt.semilogx(param, train_scores_mean, label=\"Training score\",\n",
        "              color=\"darkorange\", lw=lw)\n",
        "  plt.fill_between(param, train_scores_mean - train_scores_std,\n",
        "                  train_scores_mean + train_scores_std, alpha=0.2,\n",
        "                  color=\"darkorange\", lw=lw)\n",
        "  plt.semilogx(param, test_scores_mean, label=\"Cross-validation score\",\n",
        "              color=\"navy\", lw=lw)\n",
        "  plt.fill_between(param, test_scores_mean - test_scores_std,\n",
        "                  test_scores_mean + test_scores_std, alpha=0.2,\n",
        "                  color=\"navy\", lw=lw)\n",
        "  plt.legend(loc=\"best\")\n",
        "  plt.show()\n",
        "  return [train_scores_mean,test_scores_mean];"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4DAqA4jjc3jY",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.1.2:** Validation Curve for n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlrxH-zryQX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "# Create range of values for parameter\n",
        "param_range = np.arange(1, 250, 10)\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(RandomForestClassifier(), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"n_estimators\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7ycnGNc6wmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper(train_scores,test_scores,'n_estimators',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz7se2Vx1OG7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " visualhyper1(train_scores,test_scores,'n_estimators',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GwwWVtPQLe4g",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting. As the increase of number of decision trees, the training accuracy reaches 100% and validation accuracy converges after n reaches 10 and fluctuate around. And the accuracy have maximum around n_estimators = 60. To see where it reaches maximum we can either plot a more detailed graph or find maximum in our pltacc(mean accuracy ).\n",
        "\n",
        "There is no risk of overfitting in random forest with growing number of trees, as they are trained independently from each other. \n",
        "\n",
        "But for computing effeciency, we donot need large number of decision trees(n_estimators) as it's computational costly.\n",
        "\n",
        "Also the shaded area decrease as variance decrease as n_estimators increase in the first plot which means the variance decreases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaEyHzghOJAd",
        "colab_type": "text"
      },
      "source": [
        "Extract the param with higest accuracy and compare to our plots, we see that best number of trees should be around 50-70."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhTfsxFhPkgy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('n_estimators corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('n_estimators corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pM219HWnO7L9",
        "colab_type": "text"
      },
      "source": [
        "Check for a more detailed range:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE0MLWduN1yW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_range = np.arange(50,70, 5)\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(RandomForestClassifier(), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"n_estimators\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y4O_7PGmO0LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper(train_scores,test_scores,'n_estimators',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rzRmeW4pO4pl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('n_estimators corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7dMGbbHaddKl",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.1.3:** Validation Curve for max_depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xipn66-8z8Ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = np.arange(1, 40, 2)\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(RandomForestClassifier(), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"max_depth\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xIL0XADQKQFV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper1(train_scores,test_scores,'max_depth',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KProdM7zKGgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper(train_scores,test_scores,'max_depth',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTYBwIaZmlU3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('max_depth corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('max_depth corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5HOGy6jvhlve",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting. As the increase of max_depth, the training accuracy almost reaches 99.9% and validation accuracy converges and fluctuates around 98% after max_depth reaches 13. Also the accuracy for validation set is maximized around max_depth = 17. To see where it reaches maximum we can either plot a more detailed graph or find maximum in our pltacc(mean accuracy ). \n",
        "\n",
        "After max_depth reaches 13,  the deeper the tree, the more splits it has and it captures more information about the data and can be capable to deal with more complex problems. But it also increase the overfitting risk.In the plot, an important thing to notice is that, as the increase of max_depth, the gap between validation curve and training curve become wider which didn't show in validation curve for n_estimator.Which means the model is more overfitting as the increase of max_depth.\n",
        "\n",
        "Also the shaded area decrease as variance decrease as max_depth increase in the first plot which means the variance decreases.\n",
        "\n",
        "\n",
        "In addition, the increase of max_depth also increase computational cost. So we need to choose a suitable value of max_depth instead of focusing on accuracy only\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkRrMwxTnwBF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "param_range = np.arange(15, 30, 2)\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(RandomForestClassifier(), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"max_depth\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wu0Q6yKmD09",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper(train_scores,test_scores,'n_estimators',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpaAdxDnn8IL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('max_depth corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmoHkBhjdhp2",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.1.4:** Validation Curve for max_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FR80nIG87Au",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = ['auto','sqrt','log2']\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(RandomForestClassifier(), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"max_features\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbEOxk9prxDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'max_features',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCH0ok67sfoc",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.Max_feature is the number of features to consider each time to make the split decision.\n",
        "Increase of max_feature improve performance of model at each node, but also decreases the diversity of individual tree(Make trees similar) and increase the overfitting risk.\n",
        "\n",
        "The plot shows overfitting pattern again. As we only have 6 features to consider, three methods provide similar max_feature which don't make obvious difference on accuracy.\n",
        "\n",
        "However, 'auto' max_feature shows less overfitting pattern and highest accuracy in our graph. So 'auto' is the best choice in our case"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLYLkezN4trz",
        "colab_type": "text"
      },
      "source": [
        "### In conclusion, I think n_estimators and max_depth have bigger impact on performance. The accuracy curves of them change significantly when those two variables change. And max_depth has the greatest impact on the accuracy of our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy83G2K-wOiB",
        "colab_type": "text"
      },
      "source": [
        "## **Section 1.2:** Search for optimal hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0VyYhM5ghw-G",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.2.1:** Train a simple RandomForestClassifier model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvVzKaV118tq",
        "colab_type": "text"
      },
      "source": [
        "Train a simple model to see how RandomForestClassifier perform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5j7lPpM4RayH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "clf_rf = RandomForestClassifier(n_estimators= 100, max_depth = 3)\n",
        "clf_rf.fit(X_train,Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EStLCnvLojRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# check thee importances of all features\n",
        "print(clf_rf.feature_importances_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBHBqHtoqeCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation accuracy\n",
        "rf_pred_train = clf_rf.predict(X_train)\n",
        "rf_pred_val = clf_rf.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fg9l_bWCRayJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import scikit-learn metrics module for accuracy calculation\n",
        "from sklearn import metrics\n",
        "# Model Accuracy, how often is the classifier correct?\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(rf_pred_train, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(rf_pred_val, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ew81xlA15tE",
        "colab_type": "text"
      },
      "source": [
        "The accuracy is not that good without a good choice of hyperparameter."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbpQw6sPu-Xh",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.2.2:** 5-fold Cross Validation by RandomizedSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VqBihBFL-8E8",
        "colab_type": "text"
      },
      "source": [
        "We use RandomizedSearchCV to search for a large range and find specific parameter range for GridsearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM4jrYw0do3G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 1, stop = 300, num = 20)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt','log2']\n",
        "max_features.append(None)\n",
        "# Maximum number of levels in tree\n",
        "max_depth = np.arange(1, 60, 4)\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'bootstrap': bootstrap}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pUuVirT4J7r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "random_grid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dClcP7ZPY1Y5",
        "colab_type": "text"
      },
      "source": [
        "Random Search Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stf_rKwixdum",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# First create the base model to tune\n",
        "rf = RandomForestClassifier()\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Random search of parameters, using 5 fold cross validation, \n",
        "# search across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n",
        "                               n_iter=50,\n",
        "                               scoring='accuracy', \n",
        "                               cv = skf, verbose=2, n_jobs=-1,\n",
        "                               return_train_score=True)\n",
        "\n",
        "# Fit the random search model\n",
        "rf_random.fit(X_train, Y_train);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDz9OqO_ybR0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_random.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrkpJFsR4hR6",
        "colab_type": "text"
      },
      "source": [
        "Here the result is close to our conclusion in previous plotting part.As we only consider the accuracy, the max_depth and n_estimators will be much higher than the value it starts to converge"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo2dUNsV5Up1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction\n",
        "rf_best = rf_random.best_estimator_\n",
        "rf_pred_trainb = rf_best.predict(X_train)\n",
        "rf_pred_valb = rf_best.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDSLeT6w1gW0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation accuracy\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(rf_pred_trainb, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(rf_pred_valb, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLZkO9jO7qdY",
        "colab_type": "text"
      },
      "source": [
        "The Accuracy of RandomForest trained with best_parameters generated by stratified-k-fold randomsearched reached 100% on training set and 99.35% on vavlidation set. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGVy6ZovvDh8",
        "colab_type": "text"
      },
      "source": [
        "### **Section 1.2.3:** 5-fold Cross Validation by GridSearchCV"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqEf-B0UgfIK",
        "colab_type": "text"
      },
      "source": [
        "In this task, we use the GridSearch to do a more detailed 5-fold cross validation on hyperparameters with range including the best_parameters found by RandomizedSearchCV."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWVb_DjtvL6j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "# Create the parameter grid based on the results of grid search \n",
        "param_grid = {'n_estimators': [int(x) for x in np.linspace(start = 203, stop = 207, num = 5)],\n",
        "               'max_features': ['auto', 'sqrt','log2'],\n",
        "               'max_depth': [int(x) for x in np.linspace(35,39, num = 5)],\n",
        "               'bootstrap': bootstrap}\n",
        "# Create a based model\n",
        "rf = RandomForestClassifier()\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Instantiate the grid search model\n",
        "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,\n",
        "                          cv = skf, n_jobs = -1, verbose = 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9W0ekiZJjxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Fit the grid search to the data\n",
        "grid_search.fit(X_train, Y_train)\n",
        "grid_search.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SxBJy3OTGgks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The optimal hyperparameters are:')\n",
        "print(grid_search.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spyNiR8IJobI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prediction\n",
        "rf_best_grid = grid_search.best_estimator_\n",
        "rf_pred_grid_trainb = rf_best_grid.predict(X_train)\n",
        "rf_pred_grid_valb = rf_best_grid.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VCaN-5oEOxKc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# validation accuracy\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(rf_pred_grid_trainb, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(rf_pred_grid_valb, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCHnyvJViGJk",
        "colab_type": "text"
      },
      "source": [
        "As GridSearch range inculde the best parameter found by RandomizedSearch, the best_params found by GridSearch is our best_params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpJw0ev7KRgO",
        "colab_type": "text"
      },
      "source": [
        "The accuracy of result of GridSearch is the same as that of RandomSearch but slightly smaller. Because k-fold cross validation is a 'random' process, so its possible to have a relatively lower accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0R2DlRc5Rpsb",
        "colab_type": "text"
      },
      "source": [
        "### The optimal hyperparameters are:\n",
        "{'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'n_estimators': 207}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7nNgmKzPKD2S",
        "colab_type": "text"
      },
      "source": [
        "# **Section 2:Support Vector Machines**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtyQWS_ppBt5",
        "colab_type": "text"
      },
      "source": [
        "## **Section 2.1: Visualising different hyperparameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U05ZI-Hg-wZ8",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.1:** Validation Curve for C in linear SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-yAtG9z3SPk5",
        "colab_type": "text"
      },
      "source": [
        "As for linear kernel, we only have one hyperparamter in SVM:C"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDBU1bjpWtjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [2**k for k in range(-5,5)]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'linear'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"C\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf,\n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6th4IcaTAdKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'C',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWylA3b6Seq0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper1(train_scores,test_scores,'C',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_PWqiBqRxax",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "Here, the Accuracy increases as the increase of C before C reaches 2 and converges afterward without any imporvements in accuracy.\n",
        "\n",
        "But in that graph, accuracy stays around 89% with different values.That means the choice of different C does not make big difference(only increase of 5%). \n",
        "\n",
        "As second plot shows almost two straight lines with low accuracy, the linear model fit the data poorly compare to RandomForestClassifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kda8dK7rY2B",
        "colab_type": "text"
      },
      "source": [
        "Obviously, C is the most important hyperparameter in Linear SVM as its the only one and the change of c influence the accuracy of linear svm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mH3lf3Tx_BbP",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.2:** Validation Curve for C in poly SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIKJw5Bp9G1g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [2**k for k in range(-3,8)]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'poly'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"C\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78Wxvv5G_ss5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'C',param_range)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7la6z69mdIHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper1(train_scores,test_scores,'C',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqW1E3_udJkv",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
        "\n",
        "In the plot, the accuracy increase with the increase of penalty factor C. And the training curve converges to 100% as increase of C.\n",
        "\n",
        "However,as the increase of C.Although the accuracy increase, it lead to an increase in variance, also the overfiting risk increases.In the plot we found that the gap between training score and validation score is wider as C increases. A large C gives you low bias and high variance. Low bias because you penalize the cost of missclasification a lot.\n",
        "A small C gives you higher bias and lower variance.\n",
        "\n",
        "So we need to find a balance between bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1unTeaUd5v3E",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.3:** Validation Curve for gamma in poly SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WyraetOU_yhA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [2**k for k in range(-3,8)]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'poly'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"gamma\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "alvD794r_8TR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'gamma',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1f-h5fUjfRj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper1(train_scores,test_scores,'gamma',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MuOl_hlo_gN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('gamma in poly model corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('gamma in poly model corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pzRJbO_XnuOO",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "Gamma is the parameter of a Gaussian Kernel (to handle non-linear classification).\n",
        "In the plot, the accuracy of training and validation increase as gamma increase until gamma reach 1 and 2 respectively. But as the gap between two curves is big and donot decrease, there is overfitting risk after gamma reaches a specific value between 1 and 2. \n",
        "\n",
        "Dont like C, large Gamma will cause high bias and low variance and viceversa. We need to find a balance between Variance and Bias for this model.\n",
        "When gamma increase to a larage value, it will cause the decision hyperplane to overfit which will cause a high probability of missclassification for a new data(overfitting).But for the training line, increase of gamma will make decision hyperplane be more fitting to the training data.So the curve converges like in the graph."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrEsD6aG55hg",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.4:** Validation Curve for degree in poly SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZNCmKbhBaRn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [1,2,3,4,5,6,7]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'poly'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"degree\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVEKWmvWBaKV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'degree',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCR-8qotYrG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] =visualhyper1(train_scores,test_scores,'degree',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xEFsdr8taaN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('degree in poly model corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('degree in poly model corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPAktxyytPj_",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "Its basically the degree of the polynomial used to find the hyperplane to split the data. Higher degree allows a more flexible decision boundary and helps the SVM make an appropriate generalization, but as the degree increase to a specific level like 3 in our graph, the accuracy starts to decrease which means the SVM starts to overfit and the overfitting risk increases as the degree becomes higher and higher which cause a decrease in accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk2-pufCLgKx",
        "colab_type": "text"
      },
      "source": [
        "### **Conclusion for2.1.2&2.1.3&2.1.4:**\n",
        "\n",
        "In poly svm model, all three hyperparameters are important and we need to find a balance between bias and variance. For all three hyperparameters, we need to find a point at which accuracy converges but the model is not quite overfitting. Which will be done in the next part, which using 5-fold crossvalidation to tune the optimal parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL2b0dLH1Doo",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.5:** Validation Curve for C in rbf SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Se1To9t1BZk8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [2**k for k in range(-3,8)]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'rbf'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"C\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-awAf4_2UIE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'C',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLREiT_xRol",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] =visualhyper1(train_scores,test_scores,'C',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC0ifb60zMxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('C in rbf model corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('C in rbf model corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xZcowKzWyPYU",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "C is the penalty parameter of the error term. It controls the trade off between smooth decision boundary and classifying the training points correctly.\n",
        "\n",
        "In the plot, the accuracy increase with the increase of penalty factor C. The training curve converges to 100% after C reaches 32.The validation curve converges after reaching maximum at 16.\n",
        "\n",
        "However,as the increase of C.Although the accuracy increase, it lead to an increase in variance, the overfiting risk increases.In the plot we found that the gap between training score and validation score is wider as C increases. A large C gives low bias and high variance. Low bias because penalizing the cost of missclasification a lot. A small C gives higher bias and lower variance.\n",
        "\n",
        "So we need to find a balance between bias and variance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7AvNss76GWZ",
        "colab_type": "text"
      },
      "source": [
        "### **Section 2.1.6:** Validation Curve for gamma in rbf SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-CI2vaH2aIP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create range of values for parameter\n",
        "param_range = [2**k for k in range(-3,8)]\n",
        "skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "# Calculate accuracy on training and test set using range of parameter values\n",
        "train_scores, test_scores = validation_curve(svm.SVC(kernel = 'rbf'), \n",
        "                                             X_train, \n",
        "                                             Y_train, \n",
        "                                             param_name=\"gamma\", \n",
        "                                             param_range=param_range,\n",
        "                                             cv=skf, \n",
        "                                             scoring=\"accuracy\", \n",
        "                                             n_jobs=-1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLrxY7AR2yEn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "visualhyper(train_scores,test_scores,'gamma',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP646pr5xpJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "[trainacc,valacc] = visualhyper1(train_scores,test_scores,'gamma',param_range)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lgglzxW5tFk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('gamma in poly model corresponding to max accuracy for traing set:{}'.format(param_range[trainacc.argmax()]))\n",
        "print('gamma in poly model corresponding to max accuracy for validation set:{}'.format(param_range[valacc.argmax()]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypS77iTH5nVx",
        "colab_type": "text"
      },
      "source": [
        "The plot shows that the model is overfitting.\n",
        "\n",
        "Gamma is the parameter of a Gaussian Kernel (to handle non-linear classification).\n",
        "In the plot, the accuracy oincrease as gamma increase until gamma reach 1. But as the gap between two curves is bigger and bigger, there is overfitting risk afterwards.In the plot,the gap suddenly increases when gamma = 4 and converges to 80%.\n",
        "\n",
        "Dont like C, large Gamma will cause high bias and low variance and viceversa. We need to find a balance between Variance and Bias for this model.\n",
        "When gamma increase to a larage value, it will cause the decision hyperplane to overfit which will cause a high probability of missclassification for a new data(overfitting)so the accuracy decreases as gamma become large.But for the training line, increase of gamma will make decision hyperplane be more fitting to the training data.So the curve converges like in the graph.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzlFr1M8OvpV",
        "colab_type": "text"
      },
      "source": [
        "### **Conclusion for2.1.5&2.1.6:**\n",
        "\n",
        "In rbf svm model, both hyperparameters a and b are important and we need to find a balance between bias and variance. A large C gives low bias and high variance. Low bias because penalizing the cost of missclasification a lot. A small C gives higher bias and lower variance.By contrast,large Gamma will cause high bias and low variance and viceversa.\n",
        "As I mentioned in poly svm, we can use 5-fold crossvalidation to tune the hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BYODTKjaD97b",
        "colab_type": "text"
      },
      "source": [
        "## **Section 2.2:** Search for optimal hyperparameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ZOWWe5LZGL",
        "colab_type": "text"
      },
      "source": [
        "###**Section 2.2.1:** Train Simple SVM Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPtUkQplRXeO",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.1.1:** Train a simple linear SVM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfFxXmaMMimy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "#define linear svm model\n",
        "linear_svm = svm.SVC(kernel = 'linear')\n",
        "#train linear model with data\n",
        "linear_svm.fit(X_train,Y_train)\n",
        "#predict by model trained\n",
        "linear_svm_pred_train = linear_svm.predict(X_train)\n",
        "linear_svm_pred_test = linear_svm.predict(X_val)\n",
        "#print accuracy output\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(linear_svm_pred_train, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(linear_svm_pred_test, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yq8DeNbJT4q8",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.1.2:** Train a simple poly SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwknI8qKO8u6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define poly svm model\n",
        "poly_svm = svm.SVC(kernel = 'poly')\n",
        "#train poly svm model with data\n",
        "poly_svm.fit(X_train,Y_train)\n",
        "#predict by model trained\n",
        "poly_svm_pred_train = poly_svm.predict(X_train)\n",
        "poly_svm_pred_test = poly_svm.predict(X_val)\n",
        "#print accuracy output\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(poly_svm_pred_train, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(poly_svm_pred_test, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNOLWeynW1uF",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.1.3:** Train a simple rbf SVM Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jsqw1lAGU-NM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define rbf svm model\n",
        "rbf_svm = svm.SVC(kernel = 'rbf')\n",
        "#train poly svm model with data\n",
        "rbf_svm.fit(X_train,Y_train)\n",
        "#predict by model trained\n",
        "rbf_svm_pred_train = rbf_svm.predict(X_train)\n",
        "rbf_svm_pred_test = rbf_svm.predict(X_val)\n",
        "#print accuracy output\n",
        "print(\"Training Accuracy:\"+ str(metrics.accuracy_score(rbf_svm_pred_train, Y_train)*100)+'%')\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(rbf_svm_pred_test, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2FznUG4K6m6",
        "colab_type": "text"
      },
      "source": [
        "For simple model, rbf has highest accuracy while the linear svm model performs poorly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEJuZJ7GLwVF",
        "colab_type": "text"
      },
      "source": [
        "###**Section 2.2.2:** 5 fold Crossvalidation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImFbC2foHSkJ",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.1:** 5-fold Crossvalidation funciont"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1CmAEEU20BP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "def Random_svm_hyper_search(X,Y,Xval,Yval,ker,Crange,gammarange,degreerange):\n",
        "  # Create the random grid\n",
        "  if gammarange == 'nan':\n",
        "    randomgrid = {'C': Crange}\n",
        "  elif degreerange == 'nan':\n",
        "    randomgrid = {'C': Crange,\n",
        "                  'gamma': gammarange}\n",
        "  else:\n",
        "    randomgrid = {'C': Crange,\n",
        "                  'gamma': gammarange,\n",
        "                  'degree': degreerange}\n",
        "  svm_ran = svm.SVC(kernel = ker)\n",
        "  # Random search of parameters, using 5 fold cross validation, \n",
        "  # search across 100 different combinations, and use all available cores\n",
        "  skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "  svm_random = RandomizedSearchCV(estimator=svm_ran, param_distributions=randomgrid, cv = skf)\n",
        "  svm_random.fit(X,Y)\n",
        "  best_random = svm_random.best_estimator_\n",
        "  print(\"Training Accuracy:\"+ str(metrics.accuracy_score(best_random.predict(X), Y)*100)+'%')\n",
        "  print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(best_random.predict(Xval), Yval)*100)+'%')\n",
        "  return svm_random.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ki6C2AQqjUhd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "def Grid_svm_hyper_search(X,Y,Xval,Yval,ker,Crange,gammarange,degreerange):\n",
        "  # Create the gridSearch\n",
        "  if gammarange == 'nan':\n",
        "    grid = {'C': Crange}\n",
        "  elif degreerange == 'nan':\n",
        "    grid = {'C': Crange,\n",
        "                  'gamma': gammarange}\n",
        "  else:\n",
        "    grid = {'C': Crange,\n",
        "                  'gamma': gammarange,\n",
        "                  'degree': degreerange}\n",
        "  svm_ran = svm.SVC(kernel = ker)\n",
        "  # Grid search of parameters, using 5 fold cross validation, \n",
        "  # search across 100 different combinations, and use all available cores\n",
        "  skf = StratifiedKFold(n_splits = 5 , shuffle =True)\n",
        "  svm_grid = GridSearchCV(estimator=svm_ran, param_grid=grid, cv = skf)\n",
        "  svm_grid.fit(X,Y)\n",
        "  best_grid = svm_grid.best_estimator_\n",
        "  print(\"Training Accuracy:\"+ str(metrics.accuracy_score(best_grid.predict(X), Y)*100)+'%')\n",
        "  print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(best_grid.predict(Xval), Yval)*100)+'%')\n",
        "  return svm_grid.best_params_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTgNAxAC9SAa",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.2:** 5-fold RandomizedSearchCV for linear SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JU3-5vm_bBSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = [2**k for k in range(-4,8)]\n",
        "#gamma is not considered in linear SVM\n",
        "#degree is not considered in linear SVM\n",
        "Random_svm_hyper_search(X_train,Y_train,X_val,Y_val,'linear',C,'nan','nan')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZXlYk7gepFL",
        "colab_type": "text"
      },
      "source": [
        "Here we have optimal parameter C with value 4 for linear SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "byMgFNyYdH8h",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.3:** 5-fold GridSearchCV for linear SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvfNYUsYrloR",
        "colab_type": "text"
      },
      "source": [
        "Use the result of RandomsearchCV to gridsearchCV through a more specific range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcRV5NUdd185",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CG = [2.5,3,3.5,4,4.5,5,5.5]\n",
        "#gamma is not considered in linear SVM\n",
        "#degree is not considered in linear SVM\n",
        "Grid_svm_hyper_search(X_train,Y_train,X_val,Y_val,'linear',CG,'nan','nan')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIC_85GdgMHs",
        "colab_type": "text"
      },
      "source": [
        "Here we have optimal parameter C with value 4 for linear SVM.The accuracy donot change a lot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeQ1WFONgN0J",
        "colab_type": "text"
      },
      "source": [
        "The result follows what we expected as in our validation plot. When C increase after reaching 4 , the accuracy donot improve and even decrease somehow. In conclusion, linear model donot perform as well as other model like RandomForest as the linear SVM assumption is the way too naive."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mwi03J4n2IFE",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.4:** 5-fold RandomizedSearchCV for poly SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uYRkoN4ZlPqh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = [2**k for k in range(-3,6)]\n",
        "gamma = [2**k for k in range(-3,6)]\n",
        "degree = [1,2,3,4,5,6,7] \n",
        "Random_svm_hyper_search(X_train,Y_train,X_val,Y_val,'poly',C,gamma,degree)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81107iP-jpMe",
        "colab_type": "text"
      },
      "source": [
        "By RandomizedSearchCV, we searched the optimal parameters which are C=1, degree=5, gamma=16. Define a new range including the best parameters found by RandomizedsearchCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzxXyOyMItZm",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.5:** 5-fold GridSearchCV for poly SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5AOaTKit_7_",
        "colab_type": "text"
      },
      "source": [
        "Use the result of RandomsearchCV to gridsearchCV through a more specific range"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "37Mmk-pWhb0C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CG = [0.25,0.5,0.75,1,1.25,1.5,1.75]\n",
        "gammaG = [14,15,16,17,18]\n",
        "degreeG = [3,4,5,6,7]\n",
        "Grid_svm_hyper_search(X_train,Y_train,X_val,Y_val,'poly',CG,gammaG,degreeG)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiY-f2SawNJF",
        "colab_type": "text"
      },
      "source": [
        "The Training Accuracy didn't change, but the validation accuracy increase a lot by 4%. It shows quite good results with parameters provided by gridsearchcv."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzwpzkvAxUXx",
        "colab_type": "text"
      },
      "source": [
        "So our optimal parameters are {'C': 0.25, 'degree': 3, 'gamma': 14}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymKIGNo02SR8",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.6:** 5-fold RandomizedSearchCV for rbf SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fpCZnB862Vt1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "C = [2**k for k in range(-3,5)]\n",
        "gamma = np.logspace(-4,3,8)\n",
        "#degree is not considered in linear SVM\n",
        "Random_svm_hyper_search(X_train,Y_train,X_val,Y_val,'rbf',C,gamma,'nan')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubltz87_ymVW",
        "colab_type": "text"
      },
      "source": [
        "By RandomizedSearchCV, we searched the optimal parameters which are C=1, gamma=1.0. Define a new range including the best parameters found by RandomizedsearchCV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P38V5WnsJIS_",
        "colab_type": "text"
      },
      "source": [
        "#### **Section 2.2.2.7:** 5-fold GridSearchCV for rbf SVM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_Kl0nJK5vNg",
        "colab_type": "text"
      },
      "source": [
        "Use the result of RandomsearchCV to gridsearchCV through a more specific range\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcjcqIWZ2iNH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CG = np.linspace(1.5,2.5,num=5)\n",
        "gammaG = [0.7,0.8,0.9,1,1.1,1.2,1.3]\n",
        "#degree is not considered in linear SVM\n",
        "Grid_svm_hyper_search(X_train,Y_train,X_val,Y_val,'rbf',CG,gammaG,'nan')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9P25ixgzovH",
        "colab_type": "text"
      },
      "source": [
        "The Training Accuracy and validation accuracy didn't change. It means the hyperparameters around above range donot influence accuracy a lot. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAU_FU0hz88E",
        "colab_type": "text"
      },
      "source": [
        "So our optimal parameters are {'C': 1.5, 'gamma': 0.7}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FVlrRzHu0JmQ",
        "colab_type": "text"
      },
      "source": [
        "###**Choice of kernel in SVM:** As in our 5-fold GridSearchCV process, we find that the rbf SVM have highest accuracy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tnzb0dl0zNN",
        "colab_type": "text"
      },
      "source": [
        "###**Choice SVM:** My choice of SVM model is: 'rbf' SVM with hyperparameters:{'C': 1.5, 'gamma': 0.7}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7hTgnslZjKt",
        "colab_type": "text"
      },
      "source": [
        "# **Section 3:Neural Networks**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKXNFv0HaEqR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DZHFDd60eFmc",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3.0:**Define Neural Network function\n",
        "\n",
        "`input_size`: number of nodes in the input layer of the neural network;\n",
        "\n",
        "`hidden_size`: number of neurons in each hidden layer;\n",
        "\n",
        "`num_classes`: number of classes in MNIST;\n",
        "\n",
        "`num_epochs`: number of epochs for training;\n",
        "\n",
        "`batch_size`: number of examples the network sees before weights are updated;\n",
        "\n",
        "`learning_rate`: the learning rate $\\alpha$ in backpropagation;\n",
        "\n",
        "`val_size`: 20% of the training data goes into the validation set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOana5yC_tmM",
        "colab_type": "text"
      },
      "source": [
        "As pytorch Neural Network donot accept Y in form of DataFrame, we change the format of Y into pandas.core.series.Series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peE_GrTN_6si",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = Y_train['rating']\n",
        "y_val = Y_val['rating']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoM1nmES6oIW",
        "colab_type": "text"
      },
      "source": [
        "Define a funciton of neural network with input_size:6,hidden_size:200,num_classes:4,num_epochs:120 and chosen batchsize,learningrate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZfKq5013aKZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NNclassifier(Xtrain, Ytrain,  Xval, Yval, batchsize, learningrate):    \n",
        "      input_size = 6  \n",
        "      hidden_size = 200\n",
        "      num_classes = 4\n",
        "      num_epochs = 120\n",
        "      batch_size = batchsize\n",
        "      learning_rate = learningrate\n",
        "      # prepare data\n",
        "      xtrain = torch.from_numpy(Xtrain.values).float()\n",
        "      ytrain = torch.from_numpy(np.array(Ytrain))\n",
        "      xtest = torch.from_numpy(Xval.values).float()\n",
        "      ytest = torch.from_numpy(np.array(Yval))\n",
        "      # loading data \n",
        "      train = torch.utils.data.TensorDataset(xtrain, ytrain)\n",
        "      train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
        "      test = torch.utils.data.TensorDataset(xtest, ytest)\n",
        "      test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
        "      class NeuralNet(nn.Module):\n",
        "          def __init__(self, input_size, hidden_size, num_classes):\n",
        "              super(NeuralNet, self).__init__()\n",
        "              self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "              self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "              self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "          \n",
        "          def forward(self, x):\n",
        "              out = F.relu(self.fc1(x))\n",
        "              out = F.relu(self.fc2(out))\n",
        "              out = self.fc3(out)\n",
        "              return out\n",
        "\n",
        "      net = NeuralNet(input_size, hidden_size, num_classes)\n",
        "      # loss\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # optimiser\n",
        "      optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)  \n",
        "      total_step = len(train_loader)\n",
        "      loss_values = []\n",
        "\n",
        "      for epoch in range(num_epochs+1):\n",
        "          ###################\n",
        "          # train the model #\n",
        "          ###################\n",
        "        net.train()\n",
        "        train_loss = 0.0  \n",
        "        for i, (images, labels) in enumerate(train_loader, 0):\n",
        "          images = images\n",
        "          labels = labels\n",
        "          # labels = labels.squeeze()    \n",
        "          # forward pass\n",
        "          outputs = net(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "              \n",
        "          # backward and optimise\n",
        "          optimiser.zero_grad()\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "\n",
        "          # update loss\n",
        "          train_loss += loss.item()\n",
        "\n",
        "          # print training statistics\n",
        "          if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}] \\t Iteration [{}/{}] \\t Training Loss: {:.6f}'.format(epoch, num_epochs, i+1, total_step, train_loss / 100))\n",
        "            train_loss = 0.0\n",
        "        \n",
        "        loss_values.append(train_loss / 100)\n",
        "      #Plotting loss over epochs:\n",
        "      plt.figure(figsize=(12,8))\n",
        "      plt.ylim(0,1)\n",
        "      plt.xlim(-1, num_epochs+1)\n",
        "      plt.ylabel('Training Loss', fontsize=20)\n",
        "      plt.xlabel('Epoch', fontsize=20)\n",
        "      plt.plot(loss_values)\n",
        "      net.eval()\n",
        "      \n",
        "      #Print the accuracy of the test data\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in train_loader:\n",
        "        images = images\n",
        "        labels = labels\n",
        "        # labels = labels.squeeze()\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      print('Accuracy of the network on the train data: {} %'.format(100 * correct / total))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UZ6T04dsTRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NNclassifier(X_train,y_train,X_val,y_val,64,0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2tBKoEm8hoq",
        "colab_type": "text"
      },
      "source": [
        "Here we see,we define batchsize=64,learningrate = 0.01. The training loss decreases as the the increase of epoch we have trained and finally converge to 0. The accuracy of the network is around 96.6%."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tX5GG0JUIBkR",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3.1:**Try different learning rates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qVchsXXwIS6C",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.1.1:**Learing Rate:0.0005"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5ELZRLUBhkS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NNclassifier(X_train,y_train,X_val,y_val,64,0.0005)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NsOghEFJs2t",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.1.2:**Learing Rate:0.95"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enK-VsqmJxH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NNclassifier(X_train,y_train,X_val,y_val,64,0.95)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNWZuWRAA2rv",
        "colab_type": "text"
      },
      "source": [
        "### **Conclusion of different linearning rate:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhtIpsZ4Bc9S",
        "colab_type": "text"
      },
      "source": [
        "The weights of a neural network cannot be calculated using an analytical method. Instead, the weights must be discovered via an empirical optimization procedure called stochastic gradient descent.As mentioned in lecture, we do it by differentiate the loss function with respect to weights and multiply it by the learning rate and add it to the initial weight.(It controls the amount of apportioned error that the weights of the model are updated with each time they are updated). Then updates the weights of the model using the back-propagation of errors algorithm.\n",
        "\n",
        "Generally, Smaller learning rates require more training epochs given the smaller changes made to the weights each update, whereas larger learning rates result in rapid changes and require fewer training epochs for loss to converge. A learning rate that is too large can also cause a very quick convergence to suboptimal solution or fluctuate over training epochs and donot converge quickly.A learning rate is too small may never converge or converge to a local minimum but cannot 'jump out'.\n",
        "\n",
        "In the first plot with learning rate 0.0005. We found that the loss converges in a very slow speed and stoped around 0.2 after 120 epochs. 120 epochs are not sufficient for the loss to converge to 0, for that learning rate, we need more epochs for loss function to converge which is computationally cost.\n",
        "Also the accuracy is 74.75% which is quite low, because the loss isn't converged yet.\n",
        "\n",
        "In the second plot with learning rate 0.95, luckly the loss converges without significant jump which could be the problem for large learning rate. The convergence speed is quite quick and the accuracy is 100%. The gradient is high initially as the rate of change of weight is updated with rate 0.95.\n",
        "\n",
        "In the plot with learning rate 0.01, the curve is steeper than first plot and smooter than second plot. It shows a good convergence in loss.\n",
        "\n",
        "To find best learning rate we can only do it numerically but not analytically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhVPY4rqKJHX",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3.2:**Try different batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fraFq0ddK25H",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.2.1:**batch size:2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f72UWF0bKIy9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NNclassifier(X_train,y_train,X_val,y_val,2,0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Hj17GbGLFfb",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.2.2:**batch size:256"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7I1xODuLI_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "NNclassifier(X_train,y_train,X_val,y_val,256,0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLfyDlOQN5gC",
        "colab_type": "text"
      },
      "source": [
        "### **Conclusion of different batch size:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCg9dE5yPQSi",
        "colab_type": "text"
      },
      "source": [
        "Batch size is the number of training examples used in the estimate of the error gradient before each update of the weights of the model using the back-propagation of errors algorithm.It impacts how quickly a model learns and the stability of the learning process.\n",
        "\n",
        "Smaller batch sizes are noisy, offering a regularizing effect and lower generalization error. A larger batch will carry out a significant degradation in the quality of the model, as measured by its ability to generalize.\n",
        "\n",
        "In the plot of batch size 2, the function provide an output of training process. In which , the training accuracy tends to 0 after iteration of 20 epochs and the final accuracy is 100%.The plot converges quickly.However, it is computationally cost as it require long time to train the model.\n",
        "\n",
        "In the plot of batch size 256, the runtime of our funtion is much shorter than that of size2.The accuracy is only 89% and the plot of loss converges much slower than plot of batch size 2. The plot shows a flat line as high batch size means low updating frequency such that for each epoch, weight is updated less times like twice or once each epoch, such that the loss decrease slowly as epoch increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKVynIce_lqf",
        "colab_type": "text"
      },
      "source": [
        "## **Section 3.3:**Dropout regularisation by selfdefined function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utx2fvKHeO4_",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.3.0:**Function of Neural Network with dropout in second hidden layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKi2p6iu_rfo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def NNclassifierdrop(Xtrain, Ytrain,  Xval, Yval, batchsize, learningrate,dropoutrate):    \n",
        "      input_size = 6  \n",
        "      hidden_size = 200 \n",
        "      num_classes = 4\n",
        "      num_epochs = 120\n",
        "      batch_size = batchsize\n",
        "      learning_rate = learningrate\n",
        "      # prepare data\n",
        "      X_train = torch.from_numpy(Xtrain.values).float()\n",
        "      y_train = torch.from_numpy(np.array(Ytrain))\n",
        "      X_test = torch.from_numpy(Xval.values).float()\n",
        "      y_test = torch.from_numpy(np.array(Yval))\n",
        "      # loading data \n",
        "      train = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "      train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
        "      test = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "      test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
        "      class NeuralNet(nn.Module):\n",
        "          def __init__(self, input_size, hidden_size, num_classes):\n",
        "              super(NeuralNet, self).__init__()\n",
        "              self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "              self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "              self.dropout = torch.nn.Dropout(p= dropoutrate)\n",
        "              #p: probability of an element to be zeroed\n",
        "              self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "          def forward(self, x):\n",
        "              out = F.relu(self.fc1(x))\n",
        "              out = F.relu(self.fc2(out))\n",
        "              out = self.dropout(out)\n",
        "              out = self.fc3(out)\n",
        "              return out\n",
        "\n",
        "      net = NeuralNet(input_size, hidden_size, num_classes)\n",
        "      # loss\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "      # optimiser\n",
        "      optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)  \n",
        "      total_step = len(train_loader)\n",
        "      loss_values = []\n",
        "\n",
        "      for epoch in range(num_epochs+1):\n",
        "        net.train()\n",
        "        train_loss = 0.0  \n",
        "        for i, (images, labels) in enumerate(train_loader, 0):\n",
        "          images = images\n",
        "          labels = labels\n",
        "              \n",
        "          # forward pass\n",
        "          outputs = net(images)\n",
        "          loss = criterion(outputs, labels)\n",
        "              \n",
        "          # backward and optimise\n",
        "          optimiser.zero_grad()\n",
        "          loss.backward()\n",
        "          optimiser.step()\n",
        "\n",
        "          # update loss\n",
        "          train_loss += loss.item()\n",
        "\n",
        "          # print training statistics\n",
        "          if (i+1) % 100 == 0:\n",
        "            print('Epoch [{}/{}] \\t Iteration [{}/{}] \\t Training Loss: {:.6f}'.format(epoch, num_epochs, i+1, total_step, train_loss / 100))\n",
        "            train_loss = 0.0\n",
        "        \n",
        "        loss_values.append(train_loss / 100)\n",
        "      #Print the accuracy of the test data\n",
        "      net.eval()\n",
        "      correct = 0\n",
        "      total = 0\n",
        "      for images, labels in test_loader:\n",
        "        images = images\n",
        "        labels = labels\n",
        "        outputs = net(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "      accuracy = correct/total\n",
        "      return accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a40Hgf5be-H6",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.3.1:**Develop a selfdefined 5-fold crossvalidation method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q3lNxZe31x8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "#Define stratifiedKFold\n",
        "skf = StratifiedKFold(n_splits=5 , shuffle = True)\n",
        "#Define index for K-FOLD\n",
        "k = 0\n",
        "#Define P choice\n",
        "P = [(i)*0.1 for i in np.arange(0,11)]\n",
        "#p is the length of P\n",
        "p = len(P)\n",
        "#Define an Acc array to accept calculated data\n",
        "Acc = np.zeros((5,p))\n",
        "\n",
        "for train_index, test_index in skf.split(X_train,Y_train):\n",
        "  #Split traning and validation set in stratifiedKFold\n",
        "  train_x , test_x = X_train.iloc[train_index,:],X_train.iloc[test_index,:]\n",
        "  train_y , test_y = Y_train.iloc[train_index,:],Y_train.iloc[test_index,:]\n",
        "  for i in np.arange(p):\n",
        "    #Use defined function to calculate related accuracy\n",
        "    Acc[k][i] = NNclassifierdrop(train_x,train_y['rating'],test_x,test_y['rating'],64,0.01,P[i])\n",
        "  k=k+1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M0nIKJ1QKXHJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert Accuracy into a dataframe\n",
        "Accdf = pd.DataFrame(Acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-KuSFafKhVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Derive the mean accuracy of 5-fold stratified cross validation\n",
        "ACC = pd.DataFrame(Accdf.mean(axis = 0), columns = ['Accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fqzmoi-evMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Here the ACC represents the accuracy output corresponding to dropout rate in P provided by 5-fold stratified crossvalidation\n",
        "ACC"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tNFTFOysHJMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Dropout rate with highest accuracy:{}'.format(P[int(ACC.idxmax(axis = 0))]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnKEZ9Y0hGfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(P,ACC)\n",
        "plt.title('Accuracy dropoutrate Plot')\n",
        "plt.xlabel('Dropout Rate')\n",
        "plt.ylabel('Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80V6EA97hJlD",
        "colab_type": "text"
      },
      "source": [
        "Here we see, as the increase of dropout rate, the Accuracy decrease significantly and drop to 0,3 after dropoutrate increase to 0.9. Here the dropoutrate is the probability at which outputs of the layer are dropped out.As the increase of p, less and less neurons are trained in a layer. As lowe dropout rates(0%-40%) has similar accuracy in our case. We say the number of neurons(200) defined is good in our case.\n",
        "\n",
        "An important use of dropoutrate is to solve the problem of overfitting, the plot shows that our design of hidden layer is good which donot have an obvious overfitting pattern."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x7qdetY2ed83",
        "colab_type": "text"
      },
      "source": [
        "### **Section 3.3.2:**Dropout regularisation by selfdefined function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DDQA4IXYw7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def dropout_kfold_cv(Xtrain, Ytrain,  Xval, Yval,k,P):\n",
        "  #Define stratifiedKFold\n",
        "  skf = StratifiedKFold(n_splits=k , shuffle = True)\n",
        "  #Define index for K-FOLD\n",
        "  j = 0\n",
        "  #p is the length of P\n",
        "  p = len(P)\n",
        "  #Define an Acc array to accept calculated data\n",
        "  Acc = np.zeros((k,p))\n",
        "  for train_index, test_index in skf.split(Xtrain,Ytrain):\n",
        "    #Split traning and validation set in stratifiedKFold\n",
        "    train_x , test_x = Xtrain.iloc[train_index,:],Xtrain.iloc[test_index,:]\n",
        "    train_y , test_y = Ytrain.iloc[train_index,:],Ytrain.iloc[test_index,:]\n",
        "    for i in np.arange(p):\n",
        "      #Use defined function to calculate related accuracy\n",
        "      Acc[j][i] = NNclassifierdrop(Xtrain,Ytrain['rating'],Xval,Yval['rating'],64,0.01,P[i])\n",
        "    j=j+1\n",
        "  # Convert Accuracy into a dataframe\n",
        "  Accd = pd.DataFrame(Acc)\n",
        "  # Derive the mean accuracy of 5-fold stratified cross validation\n",
        "  ACC = pd.DataFrame(Accd.mean(axis = 0), columns = ['Accuracy'])\n",
        "  #Here the ACC represents the accuracy output corresponding to dropout rate in P provided by 5-fold stratified crossvalidation\n",
        "  optimalp = P[int(ACC.idxmax(axis = 0))]\n",
        "  return [ACC,optimalp]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ViZsQwlJbuBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# [a,b] = dropout_kfold_cv(X_train, Y_train,  X_val, Y_val,5,[(i)*0.05 for i in np.arange(0,21)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "flvYCDDSeAxP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plt.plot(P,a)\n",
        "# plt.title('Accuracy dropoutrate Plot')\n",
        "# plt.xlabel('Dropout Rate')\n",
        "# plt.ylabel('Accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tx1x8RMsQuYL",
        "colab_type": "text"
      },
      "source": [
        "# **Section 4:Discussion**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8h2lHZ6Q7NG",
        "colab_type": "text"
      },
      "source": [
        "## **Section 4.1:**Compare performance of the three classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPYERZQnJUJc",
        "colab_type": "text"
      },
      "source": [
        "Recall is defined as the number of relevant documents retrieved by a search divided by the total number of existing relevant documents.\n",
        "\n",
        "precision is defined as the number of relevant documents retrieved by a search divided by the total number of documents retrieved by that search.\n",
        "\n",
        "F1 score is the harmonic mean of the precision and recall\n",
        "\n",
        "F1 = 2*precision*recall/(precision+recall)\n",
        "\n",
        "Precision = True Positive/(True positive +False positive)\n",
        "\n",
        "Recall = True Positive/(True Posive+False Negative)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxG8zILRRM_0",
        "colab_type": "text"
      },
      "source": [
        "### **Section 4.1.1:**RandomForestClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjr93V67R67k",
        "colab_type": "text"
      },
      "source": [
        "The optimal hyperparameters are:\n",
        "{'bootstrap': False, 'max_depth': 35, 'max_features': 'auto', 'n_estimators': 207}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76c-6DMVQ5HR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import RandomForestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import confusion_matrix\n",
        "clf_rf_optimal = RandomForestClassifier(n_estimators= 207, max_depth = 35,max_features = 'auto',bootstrap='False')\n",
        "clf_rf_optimal.fit(X_train,Y_train) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sx1D5PVWSZYz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Make prediction\n",
        "rf_optimal_pred = clf_rf_optimal.predict(X_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WvL4nPIWNI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The validation accuracy of Optimal RandomForestClassifier:{}'.format(metrics.accuracy_score(rf_optimal_pred,Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zKEBjgBV5if",
        "colab_type": "text"
      },
      "source": [
        "Find the confusion matrix for RandomForest model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz3PQx5MUH_o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rf_conf = confusion_matrix(rf_optimal_pred, Y_val)\n",
        "print(rf_conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oxnWgePW-pD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(rf_conf, annot=True, ax = ax); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix for optimal RF'); \n",
        "ax.xaxis.set_ticklabels(['unacc','acc','good','vgood']); ax.yaxis.set_ticklabels(['unacc','acc','good','vgood']);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WT2F5xvvc8SL",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the numbers on diagonal is big and others are almost 0. Which means our optimal RandomForestClassifier performs quite well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6iZaRfUdnHl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accuracy_rf = metrics.accuracy_score(rf_optimal_pred, Y_val)\n",
        "Precision_rf = metrics.precision_score(rf_optimal_pred, Y_val,average='macro')\n",
        "Recall_rf = metrics.recall_score(rf_optimal_pred, Y_val,average='macro')\n",
        "F1_rf = metrics.f1_score(rf_optimal_pred, Y_val,average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0OcbkERe5sE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The validation Accuracy of our optimal RF:{}'.format(Accuracy_rf*100)+'%')\n",
        "print('The validation Precision Score of our optimal RF:{}'.format(Precision_rf*100)+'%')\n",
        "print('The validation Recall of our optimal RF:{}'.format(Recall_rf*100)+'%')\n",
        "print('The validation F1-score of our optimal RF:{}'.format(F1_rf*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPTowYDUI_f7",
        "colab_type": "text"
      },
      "source": [
        "This shows overfitting problem as validation accuracy is lower than training accuracy trained before.\n",
        "\n",
        "As we see that for Accuracy,Precision,recall and F1.All those values are similar.\n",
        "\n",
        "Precision and Recall is a measure of quality of classification to see if missclassification exists.F1 Score can be see as the combination of precision and recall. As we calculated the Precision and Recall, F1 Score seens to be repeating.\n",
        "\n",
        "In this model, the model fits data quiet well and produce almost perfect classification. So we have similar values on above measures. We only need to consider the Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLiIT6m6LB7_",
        "colab_type": "text"
      },
      "source": [
        "### **Section 4.1.2:**SVM optimal model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eiTiVKSYLJ-p",
        "colab_type": "text"
      },
      "source": [
        "Choice SVM: My choice of SVM model is: 'rbf' SVM with hyperparameters:{'C': 1.5, 'gamma': 0.7}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONz3PDNALBfd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#define rbf svm model\n",
        "clf_svm_opt = svm.SVC(kernel = 'rbf',C=1.5,gamma=0.7)\n",
        "#train poly svm model with data\n",
        "clf_svm_opt.fit(X_train,Y_train)\n",
        "#predict by model trained\n",
        "clf_svm_opt_pred = clf_svm_opt.predict(X_val)\n",
        "#print accuracy output\n",
        "print(\"Validation Accuracy:\"+ str(metrics.accuracy_score(clf_svm_opt_pred, Y_val)*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9_eSxL82MP2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "svm_conf = confusion_matrix(clf_svm_opt_pred, Y_val)\n",
        "print(svm_conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HO0RaQAMbMs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(svm_conf, annot=True, ax = ax); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix for optimal SVM'); \n",
        "ax.xaxis.set_ticklabels(['unacc','acc','good','vgood']); ax.yaxis.set_ticklabels(['unacc','acc','good','vgood']);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Xs06S9Mil5",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the numbers on diagonal is big and others are almost 0. Which means our optimal RandomForestClassifier performs quite well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dljIWbpaMlvX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accuracy_svm = metrics.accuracy_score(clf_svm_opt_pred, Y_val)\n",
        "Precision_svm = metrics.precision_score(clf_svm_opt_pred, Y_val,average='macro')\n",
        "Recall_svm = metrics.recall_score(clf_svm_opt_pred, Y_val,average='macro')\n",
        "F1_svm = metrics.f1_score(clf_svm_opt_pred, Y_val,average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXAusfzMMyj_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The validation Accuracy of our optimal SVM:{}'.format(Accuracy_svm*100)+'%')\n",
        "print('The validation Precision Score of our optimal SVM:{}'.format(Precision_svm*100)+'%')\n",
        "print('The validation Recall of our optimal SVM:{}'.format(Recall_svm*100)+'%')\n",
        "print('The validation F1-score of our optimal SVMF:{}'.format(F1_svm*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JN85Zi_iM_qY",
        "colab_type": "text"
      },
      "source": [
        "This shows overfitting problem as validation accuracy is lower than training accuracy trained before.\n",
        "\n",
        "As we see that for Accuracy,Precision,recall and F1.All those values are similar and slightly lower than those of Optimal SVM model.\n",
        "\n",
        "As mentioned before, Precision and Recall is a measure of quality of classification to see if missclassification exists.F1 Score can be see as the combination of precision and recall. As we calculated the Precision and Recall, F1 Score seens to be repeating.\n",
        "\n",
        "In this model, the model fits data quiet well and produce almost perfect classification. So we have similar values on above measures. We only need to consider the Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8gRiZZrhUWE",
        "colab_type": "text"
      },
      "source": [
        "### **Section 4.1.3:**Neural optimal model with dropoutrate = 0"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QteFE-QXjYkr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_size = 6  \n",
        "hidden_size = 200 \n",
        "num_classes = 4\n",
        "num_epochs = 120\n",
        "batch_size = 64\n",
        "learning_rate = 0.01\n",
        "# prepare data\n",
        "X_train1 = torch.from_numpy(X_train.values).float()\n",
        "y_train1 = torch.from_numpy(np.array(y_train))\n",
        "X_test1 = torch.from_numpy(X_val.values).float()\n",
        "y_test1 = torch.from_numpy(np.array(y_val))\n",
        "# loading data \n",
        "train = torch.utils.data.TensorDataset(X_train1, y_train1)\n",
        "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size)\n",
        "test = torch.utils.data.TensorDataset(X_test1, y_test1)\n",
        "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size)\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dropout = torch.nn.Dropout(p= 0)\n",
        "        #p: probability of an element to be zeroed\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes)\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.fc1(x))\n",
        "        out = F.relu(self.fc2(out))\n",
        "        out = self.dropout(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "net = NeuralNet(input_size, hidden_size, num_classes)\n",
        "# loss\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# optimiser\n",
        "optimiser = torch.optim.SGD(net.parameters(), lr=learning_rate)  \n",
        "total_step = len(train_loader)\n",
        "loss_values = []\n",
        "\n",
        "for epoch in range(num_epochs+1):\n",
        "  net.train()\n",
        "  train_loss = 0.0  \n",
        "  for i, (images, labels) in enumerate(train_loader, 0):\n",
        "    images = images\n",
        "    labels = labels\n",
        "        \n",
        "    # forward pass\n",
        "    outputs = net(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "        \n",
        "    # backward and optimise\n",
        "    optimiser.zero_grad()\n",
        "    loss.backward()\n",
        "    optimiser.step()\n",
        "\n",
        "    # update loss\n",
        "    train_loss += loss.item()\n",
        "\n",
        "    # print training statistics\n",
        "    if (i+1) % 100 == 0:\n",
        "      print('Epoch [{}/{}] \\t Iteration [{}/{}] \\t Training Loss: {:.6f}'.format(epoch, num_epochs, i+1, total_step, train_loss / 100))\n",
        "      train_loss = 0.0\n",
        "  \n",
        "  loss_values.append(train_loss / 100)\n",
        "#Print the accuracy of the test data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_-XlU4vy-jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.eval()\n",
        "\n",
        "validation_nn = []\n",
        "predict_nn = []\n",
        "for images, labels in test_loader:\n",
        "  images = images\n",
        "  labels = labels\n",
        "  outputs = net(images)\n",
        "  _, predicted = torch.max(outputs.data, 1)\n",
        "  predicted_array = predicted.numpy()\n",
        "  validation_array = labels.numpy()\n",
        "  validation_nn = np.append(validation_nn,validation_array)\n",
        "  predict_nn = np.append(predict_nn,predicted_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4wG2wffyyF0",
        "colab_type": "text"
      },
      "source": [
        "Here we have our prediction and validation set, so we can compute the confusion matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QvHeAPmyOje",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn_conf = confusion_matrix(predict_nn, validation_nn)\n",
        "print(nn_conf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6_QdF_gyQzV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ax= plt.subplot()\n",
        "sns.heatmap(nn_conf, annot=True, ax = ax); #annot=True to annotate cells\n",
        "\n",
        "# labels, title and ticks\n",
        "ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); \n",
        "ax.set_title('Confusion Matrix for NN '); \n",
        "ax.xaxis.set_ticklabels(['unacc','acc','good','vgood']); ax.yaxis.set_ticklabels(['unacc','acc','good','vgood']);\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdFWInx0OA2x",
        "colab_type": "text"
      },
      "source": [
        "Here we see that the numbers on diagonal is big and others are almost 0. Which means our optimal RandomForestClassifier performs quite well. But the missclassification in this model is more than obvious 2 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMVv5aKzNlyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Accuracy_nn = metrics.accuracy_score(predict_nn, validation_nn)\n",
        "Precision_nn = metrics.precision_score(predict_nn, validation_nn,average='macro')\n",
        "Recall_nn = metrics.recall_score(predict_nn, validation_nn,average='macro')\n",
        "F1_nn = metrics.f1_score(predict_nn, validation_nn,average='macro')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxXs_1roN_sy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('The validation Accuracy of our optimal NN:{}'.format(Accuracy_nn*100)+'%')\n",
        "print('The validation Precision Score of our optimal NN:{}'.format(Precision_nn*100)+'%')\n",
        "print('The validation Recall of our optimal NN:{}'.format(Recall_nn*100)+'%')\n",
        "print('The validation F1-score of our optimal NN:{}'.format(F1_nn*100)+'%')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bMlAyVUDOfxe",
        "colab_type": "text"
      },
      "source": [
        "The neural network model seens to have lowest accuracy and perform worst in above measures.\n",
        "\n",
        "This shows overfitting problem as validation accuracy is lower than training accuracy trained before.\n",
        "\n",
        "\n",
        "As we see that for Accuracy,Precision,recall and F1.All those values are similar and slightly lower than those of Optimal SVM model.\n",
        "\n",
        "As mentioned before, Precision and Recall is a measure of quality of classification to see if missclassification exists.F1 Score can be see as the combination of precision and recall. As we calculated the Precision and Recall, F1 Score seens to be repeating.\n",
        "\n",
        "In this model, the model fits data quiet well and produce almost perfect classification. So we have similar values on above measures. We only need to consider the Accuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC3NxQifPNbP",
        "colab_type": "text"
      },
      "source": [
        "## **Section 4.2:**Choice of our best Model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0PmyuvRPkfQ",
        "colab_type": "text"
      },
      "source": [
        "Here, we only use accuracy to measure if a model is our best choice as other measures like Precision, Recall and F1 shows similar values and properties for our model with very fitting data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6HxIffqQPCP",
        "colab_type": "text"
      },
      "source": [
        "As we donot have crossvalidation on different parameters in NeuralNetwork model. It performs worst in our case, so wo won't analyse it here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbzkkA1-Pzm8",
        "colab_type": "text"
      },
      "source": [
        "For optimal RFC:\n",
        "\n",
        "Training Accuracy:100.0%\n",
        "Validation Accuracy:99.02597402597402%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQndSFFFQEn0",
        "colab_type": "text"
      },
      "source": [
        "For optimal SVM:\n",
        "\n",
        "Training Accuracy:100.0%\n",
        "Validation Accuracy:98.37662337662337%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G6MqPCc3QbLy",
        "colab_type": "text"
      },
      "source": [
        "Both RFC and SVM have 100% accuracies on training set.\n",
        "\n",
        "As RFC is designed to solve the problem of overfitting.\n",
        "\n",
        "We see the optimal RFC have least overfitting problem, which means it fits new data better.\n",
        "\n",
        "Also the RFC has less parameters and is easier to implement, by changing tree size and tree number.\n",
        "\n",
        "Compare to other 2 models, it also cost less computationally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzdpJlj7Q5Q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}